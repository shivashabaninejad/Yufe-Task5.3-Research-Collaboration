{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# YUFE University Collaboration & Publication Analysis\n",
    "\n",
    "This project analyzes interdisciplinary collaborations and top subfields among universities using OpenAlex data.\n"
   ],
   "id": "d193d8865e4fa391"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup & Imports",
   "id": "8f46ddc5e809377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# Logging, paths, and constants\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "LOGS_DIR = Path(\"logs\")\n",
    "LOGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(LOGS_DIR / \"analysis.log\", mode='w')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ],
   "id": "753cbb4ee2b619bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Constants & Paths",
   "id": "2f81e72591f893c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Paths and filtering settings\n",
    "DATA_PATH = Path(\"data/merged_openalex_data.csv\")  # Adjust this path to your local CSV\n",
    "\n",
    "# --- Note for Reproducibility ---\n",
    "# The input CSV must contain the following columns:\n",
    "# - OPENALEX_ID\n",
    "# - Combined_University_Name\n",
    "# - DOMAIN\n",
    "# - FIELD\n",
    "# - SUBFIELD\n",
    "# - OPAL_YEAR\n",
    "# These are necessary for subsequent filtering and analysis steps.\n",
    "# The OPAL_YEAR column should contain publication years as integers.\n",
    "\n",
    "CLEANING_DIR = Path(\"data/subfield_cleaning\")\n",
    "\n",
    "# Required columns for filtering\n",
    "REQUIRED_COLUMNS = ['DOMAIN', 'FIELD', 'SUBFIELD', 'Combined_University_Name']\n",
    "\n",
    "# Year range\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2023\n"
   ],
   "id": "79d6650d53296e8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load CSV Data and Filter by Year",
   "id": "e8a277146a235938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load main dataset and filter by year\n",
    "try:\n",
    "    df_filtered_by_year = pd.read_csv(DATA_PATH)\n",
    "    df_filtered_by_year = df_filtered_by_year.query(f\"{START_YEAR} <= OPAL_YEAR <= {END_YEAR}\")\n",
    "    logger.info(f\"Loaded data with shape {df_filtered_by_year.shape} from {DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File not found: {DATA_PATH}\")\n",
    "    df_filtered_by_year = pd.DataFrame()  # Empty fallback\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Unexpected error: {e}\")\n",
    "    df_filtered_by_year = pd.DataFrame()\n"
   ],
   "id": "c5ff25ab24758af1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Subfield Cleaning Rules",
   "id": "fd65f1d65548ee26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load subfield cleaning Excel files\n",
    "excel_files = list(CLEANING_DIR.glob(\"*.xlsx\"))\n",
    "removal_list = []\n",
    "\n",
    "for file in excel_files:\n",
    "    try:\n",
    "        temp_df = pd.read_excel(file)\n",
    "        if set(REQUIRED_COLUMNS).issubset(temp_df.columns):\n",
    "            removal_list.append(temp_df[REQUIRED_COLUMNS])\n",
    "            logger.info(f\"Valid cleaning file loaded: {file.name}\")\n",
    "        else:\n",
    "            logger.warning(f\"Skipping {file.name}: missing required columns.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {file.name}: {e}\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "removal_df = pd.concat(removal_list, ignore_index=True) if removal_list else pd.DataFrame(columns=REQUIRED_COLUMNS)"
   ],
   "id": "14b059032a45572e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Apply Cleaning Filter of Irrelevant Subfields (specified by some universities)",
   "id": "7024cb486b948369"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove matching subfields\n",
    "if not df_filtered_by_year.empty:\n",
    "    df_filtered = (\n",
    "        df_filtered_by_year\n",
    "        .merge(removal_df, on=REQUIRED_COLUMNS, how='left', indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop(columns=['_merge'])\n",
    "    )\n",
    "    logger.info(f\"After subfield filtering: {df_filtered.shape[0]} rows remaining.\")\n",
    "else:\n",
    "    df_filtered = pd.DataFrame()\n",
    "    logger.warning(\"No data to filter due to previous errors.\")\n",
    "\n",
    "# Free memory\n",
    "del df_filtered_by_year\n",
    "del removal_df\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned.\")"
   ],
   "id": "9bb675b267a340d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Visualisation",
   "id": "6f684971ce449f49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize Total Unique Publications per University",
   "id": "5b79e1bba134dbbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Visualize Total Unique Publications per University\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Deduplicate publication-university pairs across fields and subfields---\n",
    "try:\n",
    "    unique_pub_uni_df = df_filtered.drop_duplicates(subset=[\"OPENALEX_ID\", \"Combined_University_Name\"])\n",
    "    logger.info(f\"Unique publication-university pairs: {len(unique_pub_uni_df)}\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during deduplication.\")\n",
    "    unique_pub_uni_df = pd.DataFrame()\n",
    "\n",
    "# --- Aggregate: Count unique publications per university ---\n",
    "try:\n",
    "    uni_publication_counts_df = (\n",
    "        unique_pub_uni_df.groupby(\"Combined_University_Name\", as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"Total_Unique_Publications\"})\n",
    "        .sort_values(\"Total_Unique_Publications\", ascending=False)\n",
    "    )\n",
    "    logger.info(\"Aggregated publication counts per university.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during aggregation.\")\n",
    "    uni_publication_counts_df = pd.DataFrame()\n",
    "\n",
    "# --- Plot ---\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(\n",
    "        uni_publication_counts_df[\"Combined_University_Name\"],\n",
    "        uni_publication_counts_df[\"Total_Unique_Publications\"],\n",
    "        color=\"royalblue\"\n",
    "    )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(\"Total Unique Publications per University\")\n",
    "    ax.set_xlabel(\"University\")\n",
    "    ax.set_ylabel(\"Total Publications\")\n",
    "    ax.set_xticks(range(len(uni_publication_counts_df)))\n",
    "    ax.set_xticklabels(uni_publication_counts_df[\"Combined_University_Name\"], rotation=90)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    logger.info(\"Bar plot generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error while plotting publication counts.\")\n",
    "\n",
    "# --- Cleanup memory ---\n",
    "del unique_pub_uni_df\n",
    "del uni_publication_counts_df\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned after visualization.\")\n"
   ],
   "id": "8123534e7bf9b861",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Aggregated DataFrame for Top Subfields per University",
   "id": "a76339de6566123e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group and rank subfields per university/domain\n",
    "try:\n",
    "    # --- Group and rank subfields ---\n",
    "    df_top_subfields_base  = (\n",
    "        df_filtered\n",
    "        .groupby(['Combined_University_Name', 'DOMAIN', 'FIELD', 'SUBFIELD'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'publication_count'})\n",
    "    )\n",
    "\n",
    "    df_top_subfields_base ['rank'] = (\n",
    "        df_top_subfields_base\n",
    "        .groupby(['Combined_University_Name', 'DOMAIN'])['publication_count']\n",
    "        .rank(method='dense', ascending=False)\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Ranked subfields for {df_top_subfields_base ['Combined_University_Name'].nunique()} universities.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during subfield ranking.\")\n",
    "    df_top_subfields_base  = pd.DataFrame()\n",
    "\n"
   ],
   "id": "bc696b353bcfc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For Sunburst (Top 3)\n",
    "df_top3_subfields = df_top_subfields_base [df_top_subfields_base ['rank'] <= 3].copy()\n",
    "\n",
    "# For Horizontal Bar Chart (Top 15)\n",
    "df_top15_subfields = df_top_subfields_base [df_top_subfields_base ['rank'] <= 15].copy()\n",
    "\n",
    "# For later use (Top 5)\n",
    "df_top5_subfields = df_top_subfields_base [df_top_subfields_base ['rank'] <= 5].copy()\n",
    "\n",
    "gc.collect()\n"
   ],
   "id": "d795bdc41df14a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extra: Collaboration analysis tables, Median subfields",
   "id": "b067a36edbe881ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================\n",
    "# SETTINGS\n",
    "# ==============================\n",
    "OUTPUT_DIR = Path(\"collaboration_tables\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: Compute median publications per subfield across all 10 YUFE universities\n",
    "# ==============================\n",
    "df_median = (\n",
    "    df_top_subfields_base\n",
    "    .groupby(['DOMAIN', 'FIELD', 'SUBFIELD'], as_index=False)['publication_count']\n",
    "    .median()\n",
    "    .rename(columns={'publication_count': 'median_publications'})\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: Filter subfields in the top 25% (high-value subfields)\n",
    "# ==============================\n",
    "top_25_cutoff = df_median['median_publications'].quantile(0.75)\n",
    "df_top_subfields = df_median[df_median['median_publications'] >= top_25_cutoff]\n",
    "\n",
    "df_with_medians = df_top_subfields_base.merge(\n",
    "    df_top_subfields,\n",
    "    on=['DOMAIN', 'FIELD', 'SUBFIELD'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Identify all universities\n",
    "all_unis = df_with_medians['Combined_University_Name'].unique()\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: Create collaboration tables for each university\n",
    "# ==============================\n",
    "uni_tables = {}\n",
    "\n",
    "for uni in all_unis:\n",
    "    df_uni = df_with_medians[df_with_medians['Combined_University_Name'] == uni].copy()\n",
    "    df_uni = df_uni[df_uni['publication_count'] >= df_uni['median_publications']]\n",
    "\n",
    "    uni_rows = []\n",
    "\n",
    "    for _, row in df_uni.iterrows():\n",
    "        subfield = row['SUBFIELD']\n",
    "        field = row['FIELD']\n",
    "        domain = row['DOMAIN']\n",
    "        uni_count = row['publication_count']\n",
    "        median_val = row['median_publications']\n",
    "\n",
    "        partners_df = df_with_medians[\n",
    "            (df_with_medians['DOMAIN'] == domain) &\n",
    "            (df_with_medians['FIELD'] == field) &\n",
    "            (df_with_medians['SUBFIELD'] == subfield) &\n",
    "            (df_with_medians['Combined_University_Name'] != uni) &\n",
    "            (df_with_medians['publication_count'] >= median_val)\n",
    "            ]\n",
    "\n",
    "        partners_list = [\n",
    "            f\"{partner} ({count})\"\n",
    "            for partner, count in zip(partners_df['Combined_University_Name'], partners_df['publication_count'])\n",
    "        ]\n",
    "\n",
    "        uni_rows.append({\n",
    "            'DOMAIN': domain,\n",
    "            'FIELD': field,\n",
    "            'SUBFIELD': subfield,\n",
    "            f\"{uni} Count\": uni_count,\n",
    "            'Median Publications': median_val,\n",
    "            'Partner Universities': \", \".join(partners_list)\n",
    "        })\n",
    "\n",
    "    if uni_rows:\n",
    "        uni_tables[uni] = pd.DataFrame(uni_rows)\n",
    "\n",
    "# ==============================\n",
    "# STEP 5: Export collaboration tables to Excel with conditional formatting\n",
    "# ==============================\n",
    "def safe_excel_sheet_name(name: str) -> str:\n",
    "    \"\"\"Create Excel-safe sheet names (<= 31 characters, no slashes).\"\"\"\n",
    "    return name.replace(\" \", \"_\").replace(\"/\", \"-\")[:31]\n",
    "\n",
    "excel_path = OUTPUT_DIR / \"potential_collaborations_top25_formatted.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "    for uni, df_uni_table in uni_tables.items():\n",
    "        df_uni_table.to_excel(writer, sheet_name=safe_excel_sheet_name(uni), index=False)\n",
    "\n",
    "        # Apply conditional formatting\n",
    "        workbook  = writer.book\n",
    "        worksheet = writer.sheets[safe_excel_sheet_name(uni)]\n",
    "\n",
    "        # Identify the column of this university's counts\n",
    "        col_idx = df_uni_table.columns.get_loc(f\"{uni} Count\")\n",
    "        col_letter = chr(ord('A') + col_idx)\n",
    "\n",
    "        # Apply a 2-color scale\n",
    "        worksheet.conditional_format(\n",
    "            f\"{col_letter}2:{col_letter}{len(df_uni_table)+1}\",\n",
    "            {'type': '2_color_scale',\n",
    "             'min_color': \"#FFFFFF\",\n",
    "             'max_color': \"#63BE7B\"}\n",
    "        )\n"
   ],
   "id": "fc87f751b6d6662d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extra: Visualise the collaboration tables",
   "id": "230dcfe80858b443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================\n",
    "# Prepare data for heatmaps per domain\n",
    "# ==============================\n",
    "domains = df_with_medians['DOMAIN'].unique()\n",
    "\n",
    "for domain in domains:\n",
    "    # Filter data for the current domain\n",
    "    df_domain = df_with_medians[df_with_medians['DOMAIN'] == domain].copy()\n",
    "\n",
    "    # Create a \"Field: Subfield\" label for y-axis\n",
    "    df_domain['Field_Subfield'] = df_domain['FIELD'] + \": \" + df_domain['SUBFIELD']\n",
    "\n",
    "    # Pivot table: subfields x universities\n",
    "    df_pivot = df_domain.pivot_table(\n",
    "        index='Field_Subfield',\n",
    "        columns='Combined_University_Name',\n",
    "        values='publication_count',\n",
    "        fill_value=0\n",
    "    )\n",
    "\n",
    "    # Sort subfields by median across universities\n",
    "    df_pivot['Median'] = df_pivot.median(axis=1)\n",
    "    df_pivot = df_pivot.sort_values(by='Median', ascending=False)\n",
    "    df_pivot = df_pivot.drop(columns='Median')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(14, max(8, len(df_pivot) * 0.3)))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.heatmap(\n",
    "        df_pivot,\n",
    "        annot=True,\n",
    "        fmt=\"g\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        cbar_kws={'label': 'Publication Count'}\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Publication Counts by Subfield and University\\nDomain: {domain}\", fontsize=16, pad=20)\n",
    "    plt.xlabel(\"University\", fontsize=12)\n",
    "    plt.ylabel(\"Field: Subfield\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "aae783b8f64e7b49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extra: collaboration table: scatter plot",
   "id": "29f5c3ef6c2575f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sunburst Chart: Top 3 Subfields per Domain (Per University)",
   "id": "955e212482630416"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# --- Generate Sunburst charts ---\n",
    "try:\n",
    "    for university, df_uni in df_top3_subfields.groupby('Combined_University_Name'):\n",
    "        fig = px.sunburst(\n",
    "            df_uni,\n",
    "            path=['DOMAIN', 'SUBFIELD'],\n",
    "            values='publication_count',\n",
    "            color='DOMAIN',\n",
    "            title=f\"Top 3 Subfields per Domain for {university}\"\n",
    "        )\n",
    "        fig.update_layout(margin=dict(t=50, l=0, r=0, b=0))\n",
    "        fig.show()\n",
    "        logger.info(f\"Sunburst chart generated for {university}.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error generating sunburst charts.\")\n"
   ],
   "id": "4e96183965235a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bar Chart: Top 15 Subfields per Domain (per University)",
   "id": "d5bd3308477a3a54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import plotly.colors as pc\n",
    "import itertools\n",
    "\n",
    "# --- Settings ---\n",
    "BAR_CHART_DIR = Path(\"subfields_top15_barcharts\")\n",
    "BAR_CHART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a color sequence that can handle up to 26 unique fields\n",
    "base_palettes = list(itertools.chain(\n",
    "    pc.qualitative.Plotly,\n",
    "    pc.qualitative.D3,\n",
    "    pc.qualitative.Set1,\n",
    "    pc.qualitative.Set2,\n",
    "    pc.qualitative.Set3,\n",
    "    pc.qualitative.Bold\n",
    "))\n",
    "if len(base_palettes) < 26:\n",
    "    base_palettes = base_palettes * ((26 // len(base_palettes)) + 1)\n",
    "base_palettes = base_palettes[:26]\n",
    "\n",
    "# Get unique fields globally for consistent FIELD color mapping\n",
    "unique_fields = df_top15_subfields['FIELD'].unique()\n",
    "field_color_map = {field: base_palettes[i % len(base_palettes)] for i, field in enumerate(unique_fields)}\n",
    "\n",
    "# Create a unique label for plotting: \"Subfield (Field)\"\n",
    "df_top15_subfields['SUBFIELD_UNIQUE'] = df_top15_subfields['SUBFIELD'] + \" (\" + df_top15_subfields['FIELD'] + \")\"\n",
    "\n",
    "# --- Generate and save bar charts ---\n",
    "for university, df_uni in df_top15_subfields.groupby(\"Combined_University_Name\"):\n",
    "    try:\n",
    "        # Sort for meaningful y-axis order\n",
    "        df_uni_sorted = df_uni.sort_values(['FIELD', 'publication_count'], ascending=[True, False])\n",
    "        subfield_order = df_uni_sorted['SUBFIELD_UNIQUE'].tolist()\n",
    "\n",
    "        # Create bar chart with FIELD-based colors\n",
    "        fig = px.bar(\n",
    "            df_uni_sorted,\n",
    "            x='publication_count',\n",
    "            y='SUBFIELD_UNIQUE',  # Use unique FIELD+SUBFIELD\n",
    "            color='FIELD',\n",
    "            color_discrete_map=field_color_map,\n",
    "            orientation='h',\n",
    "            title=f\"Top 15 Subfields per Domain for {university}\",\n",
    "            labels={\n",
    "                'publication_count': 'Publication Count',\n",
    "                'SUBFIELD_UNIQUE': 'Subfield'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            margin=dict(t=50, l=300, r=50, b=150),\n",
    "            height=1000,\n",
    "            yaxis=dict(\n",
    "                categoryorder='array',\n",
    "                categoryarray=subfield_order,\n",
    "                tickmode='array',\n",
    "                tickvals=subfield_order,\n",
    "                ticktext=subfield_order,\n",
    "                tickfont=dict(size=10)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Save chart as HTML\n",
    "        safe_filename = university.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        filepath = BAR_CHART_DIR / f\"{safe_filename}.html\"\n",
    "        fig.write_html(filepath)\n",
    "        logger.info(f\"Bar chart saved: {filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate chart for {university}: {e}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "gc.collect()\n",
    "logger.info(\"Top 15 subfield bar chart generation completed and memory cleaned.\")\n"
   ],
   "id": "4cd1cfdf53df6c39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sunburst Chart: Top 15 Subfields per Domain (per University)",
   "id": "f8e575fcdb7c9f97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Settings ---\n",
    "SUNBURST_DIR = Path(\"subfield_top15_sunburst\")\n",
    "SUNBURST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Create sunburst chart per university ---\n",
    "for university, df_uni in df_top15_subfields.groupby(\"Combined_University_Name\"):\n",
    "    try:\n",
    "        fig = px.sunburst(\n",
    "            df_uni,\n",
    "            path=['DOMAIN', 'FIELD', 'SUBFIELD'],\n",
    "            values='publication_count',\n",
    "            color='FIELD',\n",
    "            title=f\"Top 15 Subfields per Domain for {university}\",\n",
    "            labels={'publication_count': 'Publication Count'}\n",
    "        )\n",
    "\n",
    "        fig.update_layout(margin=dict(t=50, l=0, r=0, b=0))\n",
    "\n",
    "        # Safe filename and save\n",
    "        safe_filename = f\"{university.replace(' ', '_').replace('/', '-')}.html\"\n",
    "        fig.write_html(SUNBURST_DIR / safe_filename)\n",
    "        logger.info(f\"Sunburst chart saved: {SUNBURST_DIR / safe_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate sunburst for {university}: {e}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "gc.collect()\n",
    "logger.info(\"Top 15 sunburst chart generation completed and memory cleaned.\")"
   ],
   "id": "7ca15fb5a4e6e985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sunburst Chart: Top 5 Subfields per Domain (per University)",
   "id": "d19d113bb91b546a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Settings ---\n",
    "SUNBURST_DIR_TOP5 = Path(\"subfield_top5_sunburst\")\n",
    "SUNBURST_DIR_TOP5.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Loop through universities and create sunburst ---\n",
    "for university, df_uni in df_top5_subfields.groupby(\"Combined_University_Name\"):\n",
    "    try:\n",
    "        fig = px.sunburst(\n",
    "            df_uni,\n",
    "            path=['DOMAIN', 'FIELD', 'SUBFIELD'],\n",
    "            values='publication_count',\n",
    "            color='FIELD',\n",
    "            title=f\"Top 5 Subfields per Domain for {university}\",\n",
    "            labels={'publication_count': 'Publication Count'}\n",
    "        )\n",
    "\n",
    "        fig.update_layout(margin=dict(t=50, l=0, r=0, b=0))\n",
    "\n",
    "        # Safe filename and save\n",
    "        safe_filename = f\"{university.replace(' ', '_').replace('/', '-')}.html\"\n",
    "        fig.write_html(SUNBURST_DIR_TOP5 / safe_filename)\n",
    "        logger.info(f\"Top 5 Sunburst saved: {SUNBURST_DIR_TOP5 / safe_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate Top 5 sunburst for {university}: {e}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "gc.collect()\n",
    "logger.info(\"Top 5 sunburst chart generation completed and memory cleaned.\")\n"
   ],
   "id": "141c2d0cf3948a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Sunburst Chart: Universities and Domains (All Combined)",
   "id": "1f5c979bfc50870e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Settings ---\n",
    "GLOBAL_SUNBURST_DIR = Path(\"global_sunburst_html\")\n",
    "GLOBAL_SUNBURST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Deduplicate publication-domain-university level ---\n",
    "try:\n",
    "    df_domain_level = df_filtered.drop_duplicates(subset=['OPENALEX_ID', 'Combined_University_Name', 'DOMAIN'])\n",
    "    logger.info(\"Deduplicated domain-level publication data.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during deduplication at domain level.\")\n",
    "    df_domain_level = pd.DataFrame()\n",
    "\n",
    "# --- Group and calculate counts and percentages ---\n",
    "try:\n",
    "    df_grouped = (\n",
    "        df_domain_level\n",
    "        .groupby(['DOMAIN', 'Combined_University_Name'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'publication_count'})\n",
    "    )\n",
    "    df_grouped['domain_total'] = df_grouped.groupby('DOMAIN')['publication_count'].transform('sum')\n",
    "    df_grouped['percentage'] = (df_grouped['publication_count'] / df_grouped['domain_total']) * 100\n",
    "    logger.info(\"Grouped data and calculated domain totals and percentages.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error during grouping or percentage calculation.\")\n",
    "    df_grouped = pd.DataFrame()\n",
    "\n",
    "# --- Color settings ---\n",
    "domain_colors = {\n",
    "    'HEALTH': '#1f77b4',\n",
    "    'SOCIAL': '#ff7f0e',\n",
    "    'PHYSICAL': '#2ca02c',\n",
    "    'LIFE SCIENCES': '#d62728'\n",
    "}\n",
    "\n",
    "uni_colors = {\n",
    "    'University of Antwerp': '#9400D3',\n",
    "    'University of Rijeka': '#FF4500',\n",
    "    'Nicolaus Copernicus University and affiliations': '#4682B4',\n",
    "    'University of Bremen': '#008000',\n",
    "    'University of Cyprus': '#FFD700',\n",
    "    'Maastricht University and affiliations': '#006400',\n",
    "    'Universidad Carlos III de Madrid': '#8B0000',\n",
    "    'Sorbonne Nouvelle University and affiliations': '#4B0082'\n",
    "}\n",
    "\n",
    "color_map = {**domain_colors, **uni_colors}\n",
    "\n",
    "# --- Generate Sunburst ---\n",
    "try:\n",
    "    fig = px.sunburst(\n",
    "        df_grouped,\n",
    "        path=['DOMAIN', 'Combined_University_Name'],\n",
    "        values='publication_count',\n",
    "        color='Combined_University_Name',\n",
    "        color_discrete_map=color_map,\n",
    "        hover_data={'percentage': ':.1f%'},\n",
    "        title=\"Global View of Domains and Universities by Publication Count\",\n",
    "        labels={\n",
    "            'DOMAIN': 'Domain',\n",
    "            'Combined_University_Name': 'University',\n",
    "            'publication_count': 'Publication Count'\n",
    "        }\n",
    "    )\n",
    "    fig.update_traces(textinfo=\"label+percent entry\")\n",
    "    fig.update_layout(margin=dict(t=50, l=0, r=0, b=0))\n",
    "\n",
    "    output_path = GLOBAL_SUNBURST_DIR / \"global_sunburst.html\"\n",
    "    fig.write_html(output_path)\n",
    "    logger.info(f\"Global sunburst chart saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error generating global sunburst chart.\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del df_domain_level, df_grouped\n",
    "gc.collect()\n",
    "logger.info(\"Global sunburst chart completed and memory cleaned.\")"
   ],
   "id": "861c88b3ddba6b4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bubble Scatter Plot: Publications per University and Field",
   "id": "81157500a9ecc5ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Output directory and filename ---\n",
    "FIELD_SCATTER_DIR = Path(\"field_scatter\")\n",
    "FIELD_SCATTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "output_file = FIELD_SCATTER_DIR / \"Field_Scatter_Plot.html\"\n",
    "\n",
    "try:\n",
    "    # --- Deduplicate at university-field-publication level ---\n",
    "    df_field_level = df_filtered.drop_duplicates(subset=['OPENALEX_ID', 'Combined_University_Name', 'FIELD'])\n",
    "    logger.info(f\"Deduplicated to {len(df_field_level)} university-field-publication records.\")\n",
    "\n",
    "    # --- Aggregate publication counts ---\n",
    "    df_collab_field_uni = (\n",
    "        df_field_level\n",
    "        .groupby(['FIELD', 'Combined_University_Name'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'publication_count'})\n",
    "    )\n",
    "    logger.info(f\"Aggregated publication counts per field and university, total {len(df_collab_field_uni)} rows.\")\n",
    "\n",
    "    # --- Create bubble scatter plot ---\n",
    "    fig = px.scatter(\n",
    "        df_collab_field_uni,\n",
    "        x='Combined_University_Name',\n",
    "        y='FIELD',\n",
    "        size='publication_count',\n",
    "        color='Combined_University_Name',\n",
    "        labels={'publication_count': 'Number of Publications'},\n",
    "        title='Field Scatter Plot Per Institute',\n",
    "        size_max=30,\n",
    "        hover_name='FIELD',\n",
    "        hover_data={'publication_count': True}\n",
    "    )\n",
    "\n",
    "    # --- Update layout ---\n",
    "    fig.update_layout(\n",
    "        xaxis_title='University',\n",
    "        yaxis_title='Field',\n",
    "        title_x=0.5,\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        paper_bgcolor='white',\n",
    "        plot_bgcolor='white',\n",
    "        font=dict(color='black')\n",
    "    )\n",
    "\n",
    "    # --- Save the plot ---\n",
    "    fig.write_html(output_file)\n",
    "    logger.info(f\"Scatter plot saved as HTML: {output_file}\")\n",
    "\n",
    "    # --- Show the plot ---\n",
    "    fig.show()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed to create scatter plot: {e}\")\n",
    "\n",
    "\n",
    "# --- Cleanup ---\n",
    "#del df_field_level, df_collab_field_uni, fig\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned after scatter plot generation.\")\n",
    "\n"
   ],
   "id": "86ea98e38def6288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trend of Top 15 Subfields by Year (Per University)",
   "id": "12d164f29d337d5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Output directory ---\n",
    "TREND_PLOT_DIR = Path(\"subfield_trend_plots\")\n",
    "TREND_PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # --- Group and filter data ---\n",
    "    df_trend = (\n",
    "        df_filtered\n",
    "        .groupby(['Combined_University_Name', 'SUBFIELD', 'OPAL_YEAR'], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'publication_count'})\n",
    "    )\n",
    "\n",
    "    top_subfields = (\n",
    "        df_trend\n",
    "        .groupby('SUBFIELD')['publication_count']\n",
    "        .sum()\n",
    "        .nlargest(15)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    df_trend_top_15 = df_trend[df_trend['SUBFIELD'].isin(top_subfields)].copy()\n",
    "    universities = df_trend_top_15['Combined_University_Name'].unique()\n",
    "    logger.info(f\"Generating trend plots for {len(universities)} universities using top 15 subfields.\")\n",
    "\n",
    "    # Create a fixed palette for 15 subfields\n",
    "    palette = sns.color_palette(\"tab20\", 15)\n",
    "\n",
    "    for uni in universities:\n",
    "        try:\n",
    "            df_uni = df_trend_top_15[df_trend_top_15['Combined_University_Name'] == uni]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(\n",
    "                data=df_uni,\n",
    "                x='OPAL_YEAR',\n",
    "                y='publication_count',\n",
    "                hue='SUBFIELD',\n",
    "                hue_order=top_subfields,  # ensures consistent color mapping\n",
    "                marker=\"o\",\n",
    "                palette=palette\n",
    "            )\n",
    "\n",
    "            plt.title(f'Trend of Top 15 Subfields Over Time ({uni})')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Publication Count')\n",
    "            plt.legend(title='Subfield', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.xticks(range(START_YEAR, END_YEAR + 1))\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save to PNG\n",
    "            filename = f\"{uni.replace(' ', '_').replace('/', '-')}.png\"\n",
    "            filepath = TREND_PLOT_DIR / filename\n",
    "            plt.savefig(filepath, dpi=300)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved trend plot: {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate plot for {uni}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error while generating subfield trend plots.\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del df_trend, df_trend_top_15\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned after subfield trend visualization.\")\n"
   ],
   "id": "e49a99103df25589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collaboration plots",
   "id": "e2e380d2938635c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Collaboration Data Between Universities",
   "id": "2e822810a3ac265c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# --- Extract required columns ---\n",
    "try:\n",
    "    df_collab_base = df_filtered[['OPENALEX_ID', 'Combined_University_Name', 'DOMAIN']].drop_duplicates()\n",
    "    logger.info(\"Collaboration base dataset created.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to prepare collaboration base dataset.\")\n",
    "    df_collab_base = pd.DataFrame()\n",
    "\n",
    "# --- Create university pairs grouped by publication ---\n",
    "def create_uni_pairs_with_domain(group):\n",
    "    try:\n",
    "        universities = sorted(group['Combined_University_Name'].unique())\n",
    "        domain = group['DOMAIN'].iloc[0]\n",
    "        return pd.DataFrame(combinations(universities, 2), columns=['Uni1', 'Uni2']).assign(DOMAIN=domain)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Skipping group due to error: {e}\")\n",
    "        return pd.DataFrame(columns=['Uni1', 'Uni2', 'DOMAIN'])\n",
    "\n",
    "try:\n",
    "    collaboration_pairs = (\n",
    "        df_collab_base\n",
    "        .groupby('OPENALEX_ID')\n",
    "        .apply(create_uni_pairs_with_domain)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    logger.info(f\"Generated {len(collaboration_pairs)} university collaboration pairs.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to generate collaboration pairs.\")\n",
    "    collaboration_pairs = pd.DataFrame()\n",
    "\n",
    "# --- Aggregate total publications per university pair and domain ---\n",
    "try:\n",
    "    university_collaboration_domain_data = (\n",
    "        collaboration_pairs\n",
    "        .groupby(['Uni1', 'Uni2', 'DOMAIN'])\n",
    "        .size()\n",
    "        .reset_index(name='Total_Publications')\n",
    "        .sort_values(by=['Uni1', 'Uni2', 'DOMAIN'])\n",
    "    )\n",
    "    logger.info(\"Aggregated university collaboration data.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to aggregate university collaboration data.\")\n",
    "    university_collaboration_domain_data = pd.DataFrame()\n",
    "\n",
    "# Cleanup\n",
    "del df_collab_base, collaboration_pairs\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned after data prep for collaboration networks.\")\n"
   ],
   "id": "55ad885253711afd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collaboration Network Plot - Overall",
   "id": "51a334e6ca319a53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Visualize Overall University Collaboration Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    # --- Collapse domain-level collaboration to overall count ---\n",
    "    overall_collab = (\n",
    "        university_collaboration_domain_data\n",
    "        .groupby(['Uni1', 'Uni2'])\n",
    "        .agg({'Total_Publications': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    G = nx.from_pandas_edgelist(overall_collab, source='Uni1', target='Uni2', edge_attr='Total_Publications')\n",
    "\n",
    "    # Node attributes\n",
    "    total_publications = pd.concat([\n",
    "        overall_collab.groupby('Uni1')['Total_Publications'].sum(),\n",
    "        overall_collab.groupby('Uni2')['Total_Publications'].sum()\n",
    "    ]).groupby(level=0).sum().to_dict()\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    nx.set_node_attributes(G, degree_centrality, 'degree_centrality')\n",
    "\n",
    "    node_color = [total_publications.get(node, 0) for node in G.nodes()]\n",
    "    node_size = [degree_centrality.get(node, 0) * 1000 for node in G.nodes()]\n",
    "    edge_width = [G[u][v]['Total_Publications'] / 2 for u, v in G.edges()]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(22, 28))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=node_size, cmap=plt.cm.viridis_r, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5, edge_color='gray')\n",
    "\n",
    "    # Edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'Total_Publications')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=9)\n",
    "\n",
    "    # Node labels\n",
    "    node_labels = {node: f\"{node}\\nTotal: {total_publications.get(node, 0)}\" for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_color='black')\n",
    "\n",
    "    # Colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r, norm=plt.Normalize(vmin=min(node_color), vmax=max(node_color)))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=plt.gca(), label='Total Publications (Darker is Higher)')\n",
    "\n",
    "    plt.title(\"University Collaboration Network (Overall)\", fontsize=15)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    logger.info(\"Overall collaboration network plotted.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to generate overall collaboration network.\")\n"
   ],
   "id": "44ded66f3b0b8984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collaboration Network plot - per Domain",
   "id": "9f78073521c20a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "domains = university_collaboration_domain_data['DOMAIN'].unique()\n",
    "\n",
    "for domain in domains:\n",
    "    try:\n",
    "        domain_data = university_collaboration_domain_data[university_collaboration_domain_data['DOMAIN'] == domain]\n",
    "        G = nx.from_pandas_edgelist(domain_data, source='Uni1', target='Uni2', edge_attr='Total_Publications')\n",
    "\n",
    "        total_publications = pd.concat([\n",
    "            domain_data.groupby('Uni1')['Total_Publications'].sum(),\n",
    "            domain_data.groupby('Uni2')['Total_Publications'].sum()\n",
    "        ]).groupby(level=0).sum().to_dict()\n",
    "\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        nx.set_node_attributes(G, degree_centrality, 'degree_centrality')\n",
    "\n",
    "        node_color = [total_publications.get(node, 0) for node in G.nodes()]\n",
    "        node_size = [degree_centrality.get(node, 0) * 1000 for node in G.nodes()]\n",
    "        edge_width = [G[u][v]['Total_Publications'] / 2 for u, v in G.edges()]\n",
    "\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=node_color, cmap=plt.cm.viridis_r, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5, edge_color='gray')\n",
    "\n",
    "        # Formatted node labels with line breaks\n",
    "        node_labels = {\n",
    "            node: f\"{' '.join(node.split()[:2])}\\n{' '.join(node.split()[2:])}\\nTotal: {total_publications[node]}\"\n",
    "            if len(node.split()) > 2 else f\"{node}\\nTotal: {total_publications[node]}\"\n",
    "            for node in G.nodes()\n",
    "        }\n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_color='black')\n",
    "\n",
    "        edge_labels = nx.get_edge_attributes(G, 'Total_Publications')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='black', font_size=9)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r, norm=plt.Normalize(vmin=min(node_color), vmax=max(node_color)))\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, ax=plt.gca(), label='Total Publications (Darker is Higher)')\n",
    "\n",
    "        plt.title(f\"University Collaboration Network - Domain: {domain}\", fontsize=15)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        logger.info(f\"Collaboration network plotted for domain: {domain}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to generate collaboration network for domain: {domain}\")\n",
    "\n",
    "# Cleanup\n",
    "gc.collect()\n",
    "logger.info(\"Memory cleaned after domain collaboration networks.\")\n"
   ],
   "id": "6fa8f9bf0ff95edd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Additional - not part of the project",
   "id": "805592244029e79d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sankey per domain with UM label (Additional - not part of the project)",
   "id": "5eebd62014c88586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sankey diagrams with unique colors per university\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "MU_LABEL = \"Maastricht University and affiliations\"\n",
    "\n",
    "# Collect all universities\n",
    "all_unis = set(university_collaboration_domain_data['Uni1']).union(\n",
    "    set(university_collaboration_domain_data['Uni2']))\n",
    "\n",
    "# Large palette\n",
    "palette = px.colors.qualitative.Dark24 + px.colors.qualitative.Set3 + px.colors.qualitative.Pastel1\n",
    "palette_cycle = (palette * 10)\n",
    "\n",
    "# Helper to lighten a hex / rgba color\n",
    "def lighten(color, factor=0.6):\n",
    "    # factor between 0 (white) and 1 (no change)\n",
    "    if color.startswith(\"rgba\"):\n",
    "        r, g, b, a = eval(color.replace(\"rgba\", \"\"))\n",
    "        r = int(r + (255 - r) * (1 - factor))\n",
    "        g = int(g + (255 - g) * (1 - factor))\n",
    "        b = int(b + (255 - b) * (1 - factor))\n",
    "        return f\"rgba({r},{g},{b},{a})\"\n",
    "    if color.startswith(\"#\"):\n",
    "        color = color.lstrip(\"#\")\n",
    "        r = int(color[0:2], 16)\n",
    "        g = int(color[2:4], 16)\n",
    "        b = int(color[4:6], 16)\n",
    "        r = int(r + (255 - r) * (1 - factor))\n",
    "        g = int(g + (255 - g) * (1 - factor))\n",
    "        b = int(b + (255 - b) * (1 - factor))\n",
    "        return f\"rgba({r},{g},{b},1)\"\n",
    "    return color\n",
    "\n",
    "# Assign strong node colors\n",
    "color_map = {}\n",
    "i = 0\n",
    "for uni in sorted(all_unis):\n",
    "    if \"maastricht\" in uni.lower():\n",
    "        color_map[uni] = \"rgba(100,100,100,1)\"   # MU strong gray\n",
    "    else:\n",
    "        color_map[uni] = palette_cycle[i]\n",
    "        i += 1\n",
    "\n",
    "# Map the MU label explicitly\n",
    "color_map[MU_LABEL] = \"rgba(100,100,100,1)\"\n",
    "\n",
    "for domain in domains:\n",
    "    try:\n",
    "        domain_data = university_collaboration_domain_data[\n",
    "            university_collaboration_domain_data['DOMAIN'] == domain\n",
    "            ]\n",
    "\n",
    "        mu_rows = domain_data[\n",
    "            (domain_data['Uni1'].str.contains(\"Maastricht University\", case=False, na=False)) |\n",
    "            (domain_data['Uni2'].str.contains(\"Maastricht University\", case=False, na=False))\n",
    "            ]\n",
    "\n",
    "        if mu_rows.empty:\n",
    "            logger.info(f\"No MU collaborations found for domain: {domain}\")\n",
    "            continue\n",
    "\n",
    "        labels = []\n",
    "        label_index = {}\n",
    "        node_colors = []\n",
    "\n",
    "        def li(label):\n",
    "            if label not in label_index:\n",
    "                label_index[label] = len(labels)\n",
    "                labels.append(label)\n",
    "                node_colors.append(color_map[label])   # strong node color\n",
    "            return label_index[label]\n",
    "\n",
    "        sources = []\n",
    "        targets = []\n",
    "        values = []\n",
    "        link_colors = []\n",
    "\n",
    "        for _, row in mu_rows.iterrows():\n",
    "            if \"maastricht\" in row['Uni1'].lower():\n",
    "                source = MU_LABEL\n",
    "                target = row['Uni2']\n",
    "            else:\n",
    "                source = row['Uni1']\n",
    "                target = MU_LABEL\n",
    "\n",
    "            s = li(source)\n",
    "            t = li(target)\n",
    "\n",
    "            sources.append(s)\n",
    "            targets.append(t)\n",
    "            values.append(row['Total_Publications'])\n",
    "\n",
    "            # external university determines color of belt\n",
    "            ext_uni = target if source == MU_LABEL else source\n",
    "            base_color = color_map[ext_uni]\n",
    "            link_colors.append(lighten(base_color, factor=0.45))  # lighter belt\n",
    "\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node=dict(\n",
    "                pad=25,\n",
    "                thickness=20,\n",
    "                label=labels,\n",
    "                color=node_colors,        # strong color for universities\n",
    "                line=dict(color=\"rgba(0,0,0,0)\", width=0), #  no outline\n",
    "            ),\n",
    "            link=dict(\n",
    "                source=sources,\n",
    "                target=targets,\n",
    "                value=values,\n",
    "                color=link_colors        # lighter belts\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Sankey  {MU_LABEL} (Domain: {domain})\",\n",
    "            font=dict(color=\"black\", size=12)\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        logger.info(f\"Sankey diagram plotted for domain: {domain}\")\n",
    "\n",
    "    except Exception:\n",
    "        logger.exception(f\"Failed to generate Sankey diagram for domain: {domain}\")\n"
   ],
   "id": "951ecbeb20ea3ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Powerpoint reports",
   "id": "8e038cdfb71d1dc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "\n",
    "# =========================\n",
    "# SETTINGS\n",
    "# =========================\n",
    "OUTPUT_DIR = Path(\"uni_powerpoint_slides\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pptx_path = OUTPUT_DIR / \"YUFE_Collaboration_Slides.pptx\"\n",
    "\n",
    "# =========================\n",
    "# STEP 1: Compute median per field across 10 YUFE universities\n",
    "# =========================\n",
    "median_per_field = df_collab_field_uni.groupby('FIELD')['publication_count'].median().reset_index()\n",
    "median_per_field.rename(columns={'publication_count': 'median_publications'}, inplace=True)\n",
    "\n",
    "# Merge median into main dataframe\n",
    "df_input = df_collab_field_uni.merge(median_per_field, on='FIELD', how='left')\n",
    "\n",
    "# =========================\n",
    "# STEP 2: Filter fields for each university based on median\n",
    "# =========================\n",
    "uni_tables = {}\n",
    "all_unis = df_input['Combined_University_Name'].unique()\n",
    "\n",
    "for uni in all_unis:\n",
    "    df_uni = df_input[df_input['Combined_University_Name'] == uni].copy()\n",
    "    df_uni = df_uni[df_uni['publication_count'] >= df_uni['median_publications']]\n",
    "    total_pub_uni = df_uni['publication_count'].sum()\n",
    "    df_uni['Share (%)'] = df_uni['publication_count'] / total_pub_uni * 100\n",
    "\n",
    "    # # Identify partner universities per field (>= median)\n",
    "    # partner_list = []\n",
    "    # for _, row in df_uni.iterrows():\n",
    "    #     field = row['FIELD']\n",
    "    #     partners = df_input[\n",
    "    #         (df_input['FIELD'] == field) &\n",
    "    #         (df_input['Combined_University_Name'] != uni) &\n",
    "    #         (df_input['publication_count'] >= df_input['median_publications'])\n",
    "    #         ]['Combined_University_Name'].unique().tolist()\n",
    "    #     partner_list.append(\", \".join(partners))\n",
    "    #\n",
    "    # df_uni['Partner Universities'] = partner_list\n",
    "    # uni_tables[uni] = df_uni[['FIELD','publication_count','Share (%)','median_publications','Partner Universities']]\n",
    "\n",
    "    # Identify partner universities per field (>= median)\n",
    "    partner_list = []\n",
    "    for _, row in df_uni.iterrows():\n",
    "        field = row['FIELD']\n",
    "        partners_df = df_input[\n",
    "            (df_input['FIELD'] == field) &\n",
    "            (df_input['Combined_University_Name'] != uni) &\n",
    "            (df_input['publication_count'] >= df_input['median_publications'])\n",
    "            ][['Combined_University_Name','publication_count']]\n",
    "\n",
    "        partners = [f\"{p} ({c})\" for p,c in zip(partners_df['Combined_University_Name'], partners_df['publication_count'])]\n",
    "        partner_list.append(\", \".join(partners))\n",
    "\n",
    "    df_uni['Partner Universities'] = partner_list\n",
    "    uni_tables[uni] = df_uni[['FIELD','publication_count','Share (%)','median_publications','Partner Universities']]\n",
    "# =========================\n",
    "# STEP 3: Create PowerPoint\n",
    "# =========================\n",
    "prs = Presentation()\n",
    "\n",
    "for uni, df_uni_table in uni_tables.items():\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[5])  # blank slide\n",
    "    title = slide.shapes.title\n",
    "    title.text = f\"{uni} - Potential Collaborations (median-based)\"\n",
    "\n",
    "    rows, cols = df_uni_table.shape\n",
    "    rows += 1  # for header\n",
    "    table = slide.shapes.add_table(rows, cols, Inches(0.5), Inches(1.5), Inches(5), Inches(9)).table\n",
    "\n",
    "    # Set column headers\n",
    "    for j, col_name in enumerate(df_uni_table.columns):\n",
    "        table.cell(0,j).text = col_name\n",
    "        table.cell(0,j).text_frame.paragraphs[0].font.bold = True\n",
    "        table.cell(0,j).text_frame.paragraphs[0].font.size = Pt(12)\n",
    "\n",
    "    # Fill table\n",
    "    for i in range(df_uni_table.shape[0]):\n",
    "        for j in range(df_uni_table.shape[1]):\n",
    "            val = df_uni_table.iloc[i,j]\n",
    "            if isinstance(val,float):\n",
    "                val = f\"{val:.1f}\"\n",
    "            table.cell(i+1,j).text = str(val)\n",
    "            table.cell(i+1,j).text_frame.paragraphs[0].font.size = Pt(10)\n",
    "\n",
    "prs.save(pptx_path)\n",
    "print(f\"PowerPoint with 10 slides saved to {pptx_path}\")\n"
   ],
   "id": "822d7ce26828481b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7a7f364b27d368ac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
